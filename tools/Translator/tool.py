# æ–‡ä»¶è·¯å¾„: MyDataWorkbench/tools/æ–‡æ¡ˆç¿»è¯‘/tool.py

import streamlit as st
import json
import pandas as pd
import io
import os
from zipfile import ZipFile
import requests

# ----------------- å·¥å…·çš„ç§æœ‰å‡½æ•° -----------------

@st.cache_data
def _load_base_files_from_disk(base_path):
    """ä»æœ¬åœ°åŠ è½½åŸºå‡†JSONæ–‡ä»¶ã€‚"""
    if not os.path.exists(base_path):
        return None, f"é”™è¯¯ï¼šåŸºå‡†æ–‡ä»¶å¤¹ '{base_path}' ä¸å­˜åœ¨ã€‚"
    
    base_files_content = {}
    try:
        for filename in os.listdir(base_path):
            if filename.endswith('.json'):
                filepath = os.path.join(base_path, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    base_files_content[filename] = json.load(f)
        if not base_files_content:
            return None, "é”™è¯¯ï¼šåœ¨åŸºå‡†æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•.jsonæ–‡ä»¶ã€‚"
        return base_files_content, None
    except Exception as e:
        return None, f"è¯»å–åŸºå‡†æ–‡ä»¶æ—¶å‡ºé”™: {e}"

def _ai_assisted_translation(model_to_use, page_name, base_text, original_translation, target_lang):
    """
    è°ƒç”¨æœ¬åœ°Ollama APIï¼Œä½¿ç”¨ç²¾ç®€çš„æç¤ºè¯ï¼Œå¹¶å¯¹ç»“æœè¿›è¡Œå¼ºåˆ¶æ ¼å¼åŒ–æ¸…æ´—ã€‚
    """
    # â˜…â˜…â˜… æœ€ç»ˆç²¾ç®€ç‰ˆæç¤ºè¯ â˜…â˜…â˜…
    if original_translation is None or original_translation == "ã€ç¼ºå¤±ã€‘":
        prompt = f"""
TARGET LANGUAGE: {target_lang}
PAGE CONTEXT: {page_name}
SOURCE (English): "{base_text}"
"""
    else:
        prompt = f"""
TARGET LANGUAGE: {target_lang}
PAGE CONTEXT: {page_name}
SOURCE (English): "{base_text}"
CURRENT ({target_lang}): "{original_translation}"
"""

    try:
        response = requests.post(
            "http://localhost:11434/api/chat",
            json={
                "model": model_to_use, 
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "stream": False
            },
            timeout=120
        )
        response.raise_for_status()
        ai_result = response.json()['message']['content']

    except requests.exceptions.RequestException as e:
        st.error(f"è°ƒç”¨æœ¬åœ°AIå¤±è´¥: {e}ã€‚è¯·ç¡®è®¤OllamaæœåŠ¡æ­£åœ¨åå°è¿è¡Œï¼Œå¹¶ä¸”æ¨¡å‹ '{model_to_use}' å·²ç»ä¸‹è½½ã€‚")
        ai_result = "ã€AIè°ƒç”¨å¤±è´¥ã€‘"

    print(f"--- PROMPT SENT TO AI ({model_to_use}) ---\n{prompt}\n--- AI RESULT (raw) ---\n{ai_result}\n-----------------------")
    
    # â˜…â˜…â˜… ä¿ç•™æœ€å…³é”®çš„ã€100%å¯é çš„ä»£ç å±‚æ ¼å¼æ¸…æ´— â˜…â˜…â˜…
    cleaned_result = ai_result.strip()
    if len(cleaned_result) > 1 and cleaned_result.startswith('"') and cleaned_result.endswith('"'):
        cleaned_result = cleaned_result[1:-1]
    if len(cleaned_result) > 1 and cleaned_result.startswith("'") and cleaned_result.endswith("'"):
        cleaned_result = cleaned_result[1:-1]
    
    print(f"--- CLEANED RESULT ---\n{cleaned_result}\n-----------------------")
    return cleaned_result

# ----------------- å·¥å…·çš„å…¬å¼€å…¥å£å‡½æ•° -----------------
def run():
    st.subheader("æ–‡æ¡ˆä¼˜åŒ–å·¥å…·")
    st.info("æœ¬å·¥å…·ä»¥æœ¬åœ°ENæ–‡ä»¶å¤¹ä¸ºåŸºå‡†ï¼Œåˆ©ç”¨AIä¼˜åŒ–å’Œä¿®æ­£æ‚¨ä¸Šä¼ çš„ç›®æ ‡è¯­è¨€æ–‡ä»¶å¤¹ä¸­çš„æ–‡æ¡ˆã€‚")
    
    tool_dir = os.path.dirname(__file__)
    base_data_path = os.path.join(tool_dir, "data", "EN")
    base_files_content, error_msg = _load_base_files_from_disk(base_data_path)
    if error_msg:
        st.error(error_msg)
        return
    st.success(f"å·²æˆåŠŸä»æœ¬åœ°åŠ è½½ {len(base_files_content)} ä¸ªåŸºå‡† (EN) æ–‡ä»¶ï¼")
    st.markdown("---")

    st.markdown("#### 1. è¯·æŒ‡å®šæ‚¨è¦ä¼˜åŒ–çš„ç›®æ ‡è¯­è¨€")
    target_lang_name = st.text_input(
        "è¯·è¾“å…¥ç›®æ ‡è¯­è¨€çš„å…¨ç§°ï¼ˆä¾‹å¦‚: German, French, Spanish, Japanese, Chineseï¼‰:",
        value="Chinese", 
        help="AIå°†ä¼šæŠŠè‹±æ–‡æ–‡æ¡ˆä¼˜åŒ–æˆè¿™ä¸ªè¯­è¨€ã€‚"
    )
    st.markdown("---")


    st.markdown(f"#### 2. è¯·ä¸Šä¼ å¾…ä¼˜åŒ–çš„ **{target_lang_name}** è¯­è¨€æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶")
    target_files = st.file_uploader(
        "ç‚¹å‡»æŒ‰é’®åï¼Œè¿›å…¥æ–‡ä»¶å¤¹å¹¶æŒ‰ Ctrl+A å…¨é€‰æ‰€æœ‰æ–‡ä»¶", 
        type=['json'], 
        key="target_uploader",
        accept_multiple_files=True
    )
    st.markdown("---")

    if target_files:
        target_files_dict = {f.name: f for f in target_files}
        common_filenames = sorted(list(base_files_content.keys() & target_files_dict.keys()))
        
        if not common_filenames:
            st.warning("ä¸Šä¼ çš„æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸åŸºå‡†æ–‡ä»¶åŒåçš„JSONæ–‡ä»¶ã€‚")
            return

        st.info(f"æ–‡ä»¶åŒ¹é…æˆåŠŸï¼å…±æ‰¾åˆ° {len(common_filenames)} ä¸ªåŒåæ–‡ä»¶å¯ä¾›å¤„ç†ã€‚")
        
        with st.expander("ğŸ“ é¢„è§ˆä¸æ‰§è¡Œé¢æ¿", expanded=True):
            selected_file = st.selectbox("é€‰æ‹©è¦é¢„è§ˆå†…å®¹çš„æ–‡ä»¶:", common_filenames)
            
            if selected_file:
                try:
                    base_data = base_files_content[selected_file]
                    target_files_dict[selected_file].seek(0)
                    target_data = json.load(target_files_dict[selected_file])
                    
                    df = pd.DataFrame([
                        {"Key": key, "åŸºå‡†æ–‡æ¡ˆ (EN)": base_value, f"å½“å‰æ–‡æ¡ˆ ({target_lang_name})": target_data.get(key, "ã€ç¼ºå¤±ã€‘")}
                        for key, base_value in base_data.items()
                    ])
                    st.markdown(f"**æ–‡ä»¶ `'{selected_file}'` å†…å®¹é¢„è§ˆ**")
                    st.dataframe(df, use_container_width=True, height=300)

                except Exception as e:
                    st.error(f"å¤„ç†æ–‡ä»¶ '{selected_file}' æ—¶å‡ºé”™: {e}")


            if st.button("ğŸš€ ä½¿ç”¨AIæ‰¹é‡ä¼˜åŒ–æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶", use_container_width=True, type="primary"):
                current_model = st.session_state.get("selected_model", "mulebuy-optimizer") 
                st.info(f"æ­£åœ¨ä½¿ç”¨æ¨¡å‹: **{current_model}** è¿›è¡Œå¤„ç†...")

                st.session_state.ai_results = {} 
                
                with st.spinner('æ­£åœ¨è°ƒç”¨AIè¿›è¡Œæ‰¹é‡ä¼˜åŒ–...è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...'):
                    all_optimized_data = {}
                    for filename in common_filenames:
                        base_data = base_files_content[filename]
                        target_files_dict[filename].seek(0)
                        target_data = json.load(target_files_dict[filename])
                        
                        optimized_file_content = {}
                        for key, base_value in base_data.items():
                            original_translation = target_data.get(key)
                            optimized_text = _ai_assisted_translation(current_model, filename, base_value, original_translation, target_lang_name)
                            optimized_file_content[key] = optimized_text
                        
                        all_optimized_data[filename] = optimized_file_content
                
                st.session_state.ai_results = all_optimized_data
                st.success("AIä¼˜åŒ–å®Œæˆï¼è¯·åœ¨ä¸‹æ–¹å®¡æŸ¥ç»“æœã€‚")

    if 'ai_results' in st.session_state and st.session_state.ai_results:
        st.markdown("---")
        st.header("ğŸ”¬ AIä¼˜åŒ–ç»“æœå®¡æŸ¥")

        results = st.session_state.ai_results
        
        tab_filenames = list(results.keys())
        if tab_filenames:
            tab_list = st.tabs(tab_filenames)
            
            for i, filename in enumerate(tab_filenames):
                with tab_list[i]:
                    st.markdown(f"#### `{filename}` çš„ä¼˜åŒ–ç»“æœ")
                    
                    base_data = base_files_content[filename]
                    target_files_dict[filename].seek(0)
                    target_data = json.load(target_files_dict[filename])
                    optimized_data = results[filename]

                    comparison_df = pd.DataFrame([
                        {
                            "Key": key,
                            "åŸºå‡† (EN)": base_value,
                            f"åŸå§‹ ({target_lang_name})": target_data.get(key, "ã€ç¼ºå¤±ã€‘"),
                            f"AIä¼˜åŒ–å ({target_lang_name})": optimized_data.get(key)
                        } for key, base_value in base_data.items()
                    ])
                    st.dataframe(comparison_df, use_container_width=True)

            zip_buffer = io.BytesIO()
            with ZipFile(zip_buffer, 'w') as zip_file:
                for filename, content_dict in results.items():
                    zip_file.writestr(filename, json.dumps(content_dict, ensure_ascii=False, indent=4))
            
            st.download_button(
                label="ğŸ“¥ ç¡®è®¤æ— è¯¯ï¼Œä¸‹è½½åŒ…å«æ‰€æœ‰ä¼˜åŒ–åæ–‡ä»¶çš„ .zip åŒ…",
                data=zip_buffer.getvalue(),
                file_name=f"optimized_{target_lang_name}_files.zip",
                mime="application/zip",
                use_container_width=True
            )